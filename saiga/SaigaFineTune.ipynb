{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab47822-2b7f-46fb-873a-467358a7d4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84e8ff-7346-4b63-ae67-89f5c41ed7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d448f-8722-4fb8-82b9-598857e1a6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1e2d8-16dc-4e6e-ae1c-4bd21d4675b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d58e3-b40f-4f52-8752-cbc22b5aaddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install \"fsspec>=2021.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a4978-2090-4ad8-9c0c-e4d4b3f8e255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install \"s3fs>=2021.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4327095-3f1c-4a68-93a3-408d1fd4fbff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ec419-46c3-4d57-9c50-db1c33d82195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install transformers[sentencepiece]\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe5497-5b08-44be-9982-9871558f80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers==4.36.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0263498-b413-457e-9769-cbd3b07d5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "!git clone https://github.com/IlyaGusev/rulm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b6f81-b6e6-4541-92d6-52d3bbdd442c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip uninstall xxhash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91af6fd-e80e-48cb-bb99-82c34427ffac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T09:29:23.255481Z",
     "iopub.status.busy": "2024-05-21T09:29:23.255027Z",
     "iopub.status.idle": "2024-05-21T09:29:23.272442Z",
     "shell.execute_reply": "2024-05-21T09:29:23.271753Z",
     "shell.execute_reply.started": "2024-05-21T09:29:23.255458Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my friend\n"
     ]
    }
   ],
   "source": [
    "print('Hello my friend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b0ed43-3768-4e54-98e2-00107009ab1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T08:59:53.018721Z",
     "iopub.status.busy": "2024-05-21T08:59:53.018429Z",
     "iopub.status.idle": "2024-05-21T08:59:53.046941Z",
     "shell.execute_reply": "2024-05-21T08:59:53.046215Z",
     "shell.execute_reply.started": "2024-05-21T08:59:53.018702Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayoooo\n"
     ]
    }
   ],
   "source": [
    "print('Ayoooo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29718b8e-d137-4b9c-95d2-8b6e750ce0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:08:34.997708Z",
     "iopub.status.busy": "2024-05-21T22:08:34.997103Z",
     "iopub.status.idle": "2024-05-21T22:11:54.867885Z",
     "shell.execute_reply": "2024-05-21T22:11:54.866937Z",
     "shell.execute_reply.started": "2024-05-21T22:08:34.997682Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 22:08:38.882228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfb11e2-4d17-4735-930a-00801d1d6baf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.870018Z",
     "iopub.status.busy": "2024-05-21T22:11:54.869258Z",
     "iopub.status.idle": "2024-05-21T22:11:54.883033Z",
     "shell.execute_reply": "2024-05-21T22:11:54.882413Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.869995Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from peft import PeftModel, PeftConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForTokenClassification, AutoConfig, GenerationConfig\n",
    "from transformers import Trainer, TrainingArguments, logging, TrainerCallback, TrainerState, TrainerControl, BitsAndBytesConfig\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ac16d0-72ca-48aa-87e8-f743b885efad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.884013Z",
     "iopub.status.busy": "2024-05-21T22:11:54.883704Z",
     "iopub.status.idle": "2024-05-21T22:11:54.893650Z",
     "shell.execute_reply": "2024-05-21T22:11:54.892986Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.883994Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "from typing import Any, List, Mapping, Optional\n",
    "import transformers\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ea1888-1d5a-458a-b878-98930e999ab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.895797Z",
     "iopub.status.busy": "2024-05-21T22:11:54.895242Z",
     "iopub.status.idle": "2024-05-21T22:11:54.914499Z",
     "shell.execute_reply": "2024-05-21T22:11:54.913823Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.895777Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayoooo\n"
     ]
    }
   ],
   "source": [
    "print('Ayoooo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dde20dc-08dd-48f6-bac4-52bf640b294b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.915390Z",
     "iopub.status.busy": "2024-05-21T22:11:54.915085Z",
     "iopub.status.idle": "2024-05-21T22:11:54.930808Z",
     "shell.execute_reply": "2024-05-21T22:11:54.930085Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.915371Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        start_token_id=1,\n",
    "        bot_token_id=9225\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.start_token_id = start_token_id\n",
    "        self.bot_token_id = bot_token_id\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def get_start_token_id(self):\n",
    "        return self.start_token_id\n",
    "\n",
    "    def get_bot_token_id(self):\n",
    "        return self.bot_token_id\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c991ccf1-b5c0-4d68-8b82-50ca8555a986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.931828Z",
     "iopub.status.busy": "2024-05-21T22:11:54.931557Z",
     "iopub.status.idle": "2024-05-21T22:11:54.940369Z",
     "shell.execute_reply": "2024-05-21T22:11:54.939768Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.931809Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayoooo\n"
     ]
    }
   ],
   "source": [
    "print('Ayoooo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75946de-7c8a-4045-b2d7-186a7c1f1d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef2acc-9fc3-4c34-9777-015c36a14f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('helo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d183ef-d954-4eb3-ac61-9378e0f403b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ce06d-dcb6-4b1d-a6d4-b67bb855a3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be76507-c7f9-4a7f-b53a-e29bdf4dd72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150be29-ec13-4e57-aa57-c5de1cada7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc8d55-e6d2-496a-86c7-f5a65f872b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "inp = 'Почему трава зеленая?'\n",
    "conversation = Conversation(system_prompt='Отвечай')\n",
    "conversation.add_user_message(inp)\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7050f5-4ff6-4919-bd64-78809d9ffd83",
   "metadata": {},
   "source": [
    "-------------------------------------------------\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a31c6f-9d2a-4e30-bf2a-4106b2b15ea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.941323Z",
     "iopub.status.busy": "2024-05-21T22:11:54.941050Z",
     "iopub.status.idle": "2024-05-21T22:11:54.950588Z",
     "shell.execute_reply": "2024-05-21T22:11:54.949990Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.941304Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a441307-a58b-40cc-81e2-cc09bc6b79f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.951748Z",
     "iopub.status.busy": "2024-05-21T22:11:54.951301Z",
     "iopub.status.idle": "2024-05-21T22:11:54.964161Z",
     "shell.execute_reply": "2024-05-21T22:11:54.963337Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.951728Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from peft import AutoPeftModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ec0d40-2039-4c69-a0ec-d678eac7050d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.965126Z",
     "iopub.status.busy": "2024-05-21T22:11:54.964841Z",
     "iopub.status.idle": "2024-05-21T22:11:54.974783Z",
     "shell.execute_reply": "2024-05-21T22:11:54.974201Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.965107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForTokenClassification, AutoConfig, GenerationConfig\n",
    "from transformers import Trainer, TrainingArguments, logging, TrainerCallback, TrainerState, TrainerControl, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c8e6bd2-3add-4ec7-858e-d49e65e70af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.977456Z",
     "iopub.status.busy": "2024-05-21T22:11:54.977170Z",
     "iopub.status.idle": "2024-05-21T22:11:54.984979Z",
     "shell.execute_reply": "2024-05-21T22:11:54.984385Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.977436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66590401-039b-4923-b31b-710b84340146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.985875Z",
     "iopub.status.busy": "2024-05-21T22:11:54.985610Z",
     "iopub.status.idle": "2024-05-21T22:11:54.993865Z",
     "shell.execute_reply": "2024-05-21T22:11:54.993270Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.985856Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76f5237b-cea3-4a00-8642-b5f36f4a00a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:11:54.995043Z",
     "iopub.status.busy": "2024-05-21T22:11:54.994550Z",
     "iopub.status.idle": "2024-05-21T22:11:55.003567Z",
     "shell.execute_reply": "2024-05-21T22:11:55.002976Z",
     "shell.execute_reply.started": "2024-05-21T22:11:54.995023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "import transformers\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00f3c64d-c405-439b-87d7-1f38aca17d46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:20:26.158793Z",
     "iopub.status.busy": "2024-05-21T22:20:26.157047Z",
     "iopub.status.idle": "2024-05-21T22:20:35.008053Z",
     "shell.execute_reply": "2024-05-21T22:20:35.007200Z",
     "shell.execute_reply.started": "2024-05-21T22:20:26.158763Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      " 86%|████████▌ | 171/200 [07:05<01:12,  2.49s/it] 0:00<?, ?it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 3584,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.2,\n",
      "  \"temperature\": 0.5,\n",
      "  \"top_k\": 30,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "Прошло времени 8.824531316757202\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "st_time = time.time()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_4bit = True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    is_trainable = True\n",
    ").to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    print('Count: ', torch.cuda.device_count())\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "print(f'Прошло времени {time.time() - st_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa7e427-4876-46c0-aa63-0ecf5ac5fde5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:05.676713Z",
     "iopub.status.busy": "2024-05-21T22:12:05.676369Z",
     "iopub.status.idle": "2024-05-21T22:12:05.695868Z",
     "shell.execute_reply": "2024-05-21T22:12:05.695021Z",
     "shell.execute_reply.started": "2024-05-21T22:12:05.676691Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c5055a-19c2-435a-b1b7-eb08af60b19f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:05.697143Z",
     "iopub.status.busy": "2024-05-21T22:12:05.696857Z",
     "iopub.status.idle": "2024-05-21T22:12:05.715890Z",
     "shell.execute_reply": "2024-05-21T22:12:05.715214Z",
     "shell.execute_reply.started": "2024-05-21T22:12:05.697124Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5588f98e-c872-4545-810e-c7293a301c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:05.716917Z",
     "iopub.status.busy": "2024-05-21T22:12:05.716652Z",
     "iopub.status.idle": "2024-05-21T22:12:05.738431Z",
     "shell.execute_reply": "2024-05-21T22:12:05.737723Z",
     "shell.execute_reply.started": "2024-05-21T22:12:05.716898Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='TheBloke/Llama-2-7B-fp16', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=16, target_modules={'o_proj', 'k_proj', 'v_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "282a3547-775a-4b10-b2b3-b880afb5ba09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:05.740271Z",
     "iopub.status.busy": "2024-05-21T22:12:05.739165Z",
     "iopub.status.idle": "2024-05-21T22:12:05.756943Z",
     "shell.execute_reply": "2024-05-21T22:12:05.756419Z",
     "shell.execute_reply.started": "2024-05-21T22:12:05.740248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16777216 || all params: 3517190144 || trainable%: 0.477006226934315\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7b884aa-b77b-46f6-a7be-87e3d8df2fed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:05.758084Z",
     "iopub.status.busy": "2024-05-21T22:12:05.757784Z",
     "iopub.status.idle": "2024-05-21T22:12:21.081553Z",
     "shell.execute_reply": "2024-05-21T22:12:21.080927Z",
     "shell.execute_reply.started": "2024-05-21T22:12:05.758065Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему трава зеленая?\n",
      "Зеленый цвет травы связан с наличием в ней хлорофиллов. Хлорофилл - это пигмент, который обеспечивает защиту растения от ультрафиолетового излучения солнца и способствует фотосинтезу. Зельное окрашивание травы происходит благодаря тому, что хлорофиллам удается сохранять свои свойства при дневных температурах воздуха выше 20-30 градусов Цельсия (в зависимости от видов), тогда как другие пигменты быстро разрушаются под действием тепла. Это позволяет травянистым растениям выживать в условиях жаркого климата и получать необходимость энергии для роста и развития.\n",
      "CPU times: user 15.2 s, sys: 112 ms, total: 15.3 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inp = 'Почему трава зеленая?'\n",
    "conversation = Conversation(system_prompt='Отвечай')\n",
    "conversation.add_user_message(inp)\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e13ceb-1524-43e9-9872-0e75bc658070",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4326fb7d-3cc2-4b4c-8dee-69161eb90756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.082754Z",
     "iopub.status.busy": "2024-05-21T22:12:21.082437Z",
     "iopub.status.idle": "2024-05-21T22:12:21.093082Z",
     "shell.execute_reply": "2024-05-21T22:12:21.092476Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.082730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87107bbc-e10f-43c1-8869-4c3102801d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.093954Z",
     "iopub.status.busy": "2024-05-21T22:12:21.093677Z",
     "iopub.status.idle": "2024-05-21T22:12:21.724136Z",
     "shell.execute_reply": "2024-05-21T22:12:21.723249Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.093935Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100 examples [00:00, 6582.29 examples/s]\n",
      "Generating validation split: 100 examples [00:00, 19007.13 examples/s]\n",
      "Generating test split: 1045 examples [00:00, 28222.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['bot', 'system', 'user'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['bot', 'system', 'user'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['bot', 'system', 'user'],\n",
       "        num_rows: 1045\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\n",
    "    \"json\", \n",
    "    data_files={'train' : '/home/jupyter/datasphere/project/val_real.json' , 'validation' : '/home/jupyter/datasphere/project/val_real.json', 'test' : '/home/jupyter/datasphere/project/test.json'}\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fca93f47-dac5-4371-8b46-064fe9b5e322",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.725326Z",
     "iopub.status.busy": "2024-05-21T22:12:21.724923Z",
     "iopub.status.idle": "2024-05-21T22:12:21.747Z",
     "shell.execute_reply": "2024-05-21T22:12:21.746136Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.725304Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "<s>\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.decode([1]))\n",
    "print(tokenizer.decode([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4f69c7d-b862-4cc4-a3df-b52611bb1720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.747996Z",
     "iopub.status.busy": "2024-05-21T22:12:21.747630Z",
     "iopub.status.idle": "2024-05-21T22:12:21.762104Z",
     "shell.execute_reply": "2024-05-21T22:12:21.761404Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.747976Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1788], 'attention_mask': [1]}\n",
      "{'input_ids': [1404], 'attention_mask': [1]}\n",
      "{'input_ids': [9225], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('system', add_special_tokens=False))\n",
    "print(tokenizer('user', add_special_tokens=False))\n",
    "print(tokenizer('bot', add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c70e83f3-71d0-45de-9860-a2145e65ac5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.763515Z",
     "iopub.status.busy": "2024-05-21T22:12:21.762985Z",
     "iopub.status.idle": "2024-05-21T22:12:21.778869Z",
     "shell.execute_reply": "2024-05-21T22:12:21.778127Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.763495Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1, 1788, 1695, 7616, 5413, 24280, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('<s>system привет как дела</s><s>',truncation=True,\n",
    "        max_length=3584,\n",
    "        padding=False,\n",
    "        return_tensors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0836a37d-fe5d-4c58-ad2b-8b9d3f563b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.780392Z",
     "iopub.status.busy": "2024-05-21T22:12:21.779535Z",
     "iopub.status.idle": "2024-05-21T22:12:21.802226Z",
     "shell.execute_reply": "2024-05-21T22:12:21.801267Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.780372Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: 1788\n",
      "user: 1404\n",
      "bot: 9225\n"
     ]
    }
   ],
   "source": [
    "CUTOFF_LEN = 3584\n",
    "\n",
    "SYSTEM = tokenizer('system', add_special_tokens=False)['input_ids'][0]\n",
    "USER = tokenizer('user', add_special_tokens=False)['input_ids'][0]\n",
    "BOT = tokenizer('bot', add_special_tokens=False)['input_ids'][0]\n",
    "print(f'system: {SYSTEM}')\n",
    "print(f'user: {USER}')\n",
    "print(f'bot: {BOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "188cc198-e659-4e6d-a6d1-9239c89399dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.803734Z",
     "iopub.status.busy": "2024-05-21T22:12:21.802863Z",
     "iopub.status.idle": "2024-05-21T22:12:21.820885Z",
     "shell.execute_reply": "2024-05-21T22:12:21.820291Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.803705Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 3584\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    promt = f\"\"\"<s>system\n",
    "{data_point['system']}</s><s>user\n",
    "{data_point['user']}</s><s>bot\n",
    "{data_point['bot']}</s>\"\"\"\n",
    "    #     print(promt)\n",
    "    return promt\n",
    " \n",
    "    \n",
    "def tokenize (prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        \n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    " \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "#     print(tokenized_full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "870ae614-ea45-4d5a-9370-d8bceedb6501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.822245Z",
     "iopub.status.busy": "2024-05-21T22:12:21.821580Z",
     "iopub.status.idle": "2024-05-21T22:12:21.858583Z",
     "shell.execute_reply": "2024-05-21T22:12:21.857753Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.822221Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 1, 1788, 13, 30054, 29972, 29942, 753, 717, 1097, 7678, 3378, 3327, 18077, 494, 490, 9116, 1155, 730, 4390, 1866, 614, 7832, 1587, 606, 614, 29969, 5206, 6052, 23092, 24871, 665, 8487, 27268, 29964, 6188, 1502, 29889, 939, 1685, 7616, 2771, 1521, 956, 1097, 7678, 3378, 3327, 18077, 494, 606, 11244, 6253, 12395, 29889, 2, 1, 1404, 13, 30038, 29969, 5206, 6195, 29901, 6083, 29960, 5460, 379, 3357, 26599, 350, 29941, 29896, 29896, 29899, 29906, 29906, 29896, 531, 1447, 9518, 25163, 2082, 21944, 30002, 8150, 14519, 12087, 2082, 718, 1325, 29959, 10766, 676, 13, 30038, 7832, 1755, 29901, 29871, 2014, 4222, 24178, 7372, 2370, 16042, 11744, 29901, 14570, 29899, 18800, 29899, 576, 29960, 5460, 718, 1447, 9518, 25163, 3162, 14519, 12087, 477, 29871, 29945, 757, 718, 20468, 29899, 15266, 676, 29889, 6083, 29960, 5460, 26780, 19945, 29932, 3807, 21656, 717, 317, 7833, 29899, 15266, 3327, 606, 20302, 1604, 4449, 2100, 1499, 1413, 25827, 24050, 29899, 1268, 29957, 8776, 12854, 1447, 29871, 29941, 29900, 836, 4449, 29964, 7735, 29957, 2430, 863, 21237, 2513, 3288, 2835, 13145, 570, 29889, 13, 13, 30082, 702, 29932, 16042, 11744, 14367, 1084, 4820, 16970, 1909, 2237, 8751, 3807, 2771, 26352, 490, 25827, 24050, 490, 11425, 27596, 29988, 29892, 11270, 1685, 20684, 25251, 18493, 29957, 2082, 25827, 24050, 29889, 6083, 29960, 5460, 26400, 13204, 1257, 6457, 1500, 5413, 531, 18493, 29957, 4470, 25827, 821, 4816, 29892, 4553, 606, 531, 2569, 2384, 693, 4470, 25827, 821, 4816, 13068, 20468, 29899, 15266, 1500, 29889, 20782, 9253, 21487, 570, 20356, 1500, 4393, 570, 29892, 1694, 12068, 18493, 29957, 2082, 25827, 24050, 19568, 1257, 531, 2942, 1782, 26344, 7082, 490, 516, 1502, 1685, 20684, 25251, 29889, 13, 13, 30027, 29904, 29960, 5460, 26400, 13204, 1257, 1586, 811, 29871, 29941, 29954, 29892, 29871, 29946, 29954, 29892, 365, 4330, 29871, 29946, 28868, 588, 5887, 606, 26780, 29871, 29906, 21463, 676, 29901, 365, 2190, 606, 399, 2190, 29889, 16456, 6517, 26400, 13204, 1257, 16223, 29959, 1909, 1668, 6713, 606, 14887, 2584, 2542, 24712, 693, 29889, 26595, 5508, 1561, 29960, 6713, 29901, 662, 16279, 29871, 29896, 29947, 15800, 29892, 14141, 24436, 29871, 29896, 29941, 15800, 29892, 15804, 676, 29871, 29906, 15800, 29889, 13, 13, 30012, 16042, 13009, 730, 531, 1561, 29960, 730, 4416, 28866, 29932, 21944, 30002, 23620, 863, 1325, 2510, 3162, 14519, 12087, 477, 531, 3212, 30055, 29919, 17795, 317, 1529, 606, 662, 644, 2082, 1787, 3759, 1225, 29871, 29945, 17194, 2899, 29889, 7473, 29959, 29899, 15266, 676, 2102, 1802, 3843, 1225, 4364, 490, 12493, 16635, 29892, 606, 1710, 641, 30011, 19568, 1257, 733, 6495, 29977, 11325, 665, 4724, 4791, 16458, 9011, 10744, 753, 29906, 490, 531, 1257, 13053, 29871, 29941, 29954, 29914, 29871, 29946, 29954, 29889, 7127, 1802, 3843, 1225, 4364, 25827, 24050, 29899, 6711, 2885, 29951, 29871, 29906, 29900, 29900, 1618, 30031, 10642, 614, 2194, 9578, 2332, 733, 18924, 576, 1415, 606, 665, 3212, 840, 9797, 29892, 18068, 5719, 29932, 1538, 1787, 1282, 1257, 29889, 1291, 1782, 29921, 3648, 4484, 8433, 676, 21500, 29871, 29929, 29900, 29900, 5368, 29975, 6188, 490, 29706, 29996, 29889, 13, 13, 30013, 1719, 29957, 17442, 8751, 23842, 4647, 1630, 3693, 15757, 956, 1413, 11650, 3029, 665, 7391, 2937, 12394, 1779, 29988, 29889, 20782, 20302, 23124, 1413, 1685, 24356, 1447, 8847, 29988, 6457, 1499, 29988, 662, 8150, 29889, 12485, 3693, 15757, 18479, 11650, 2430, 26634, 733, 9518, 507, 1413, 4724, 4453, 29935, 606, 7140, 1413, 18636, 9718, 4199, 25827, 821, 4816, 29889, 6049, 4453, 29935, 665, 10766, 730, 21500, 29871, 29900, 5368, 29975, 6188, 29889, 13, 13, 20093, 2711, 1844, 29977, 1229, 29892, 1538, 20345, 29978, 730, 11366, 12735, 30005, 614, 14236, 1382, 1345, 5392, 2237, 22951, 29889, 2081, 570, 9077, 22568, 29932, 733, 11414, 29975, 1263, 4199, 20658, 2510, 3029, 665, 21237, 717, 29892, 606, 26907, 23618, 29959, 1345, 4184, 14752, 19943, 8935, 29959, 13068, 3212, 1216, 29944, 376, 27129, 5945, 5551, 29908, 7082, 11425, 1268, 29957, 6620, 29998, 1382, 29889, 2, 1, 9225, 13, 29912, 376, 30017, 29904, 13538, 1115, 376, 29950, 3357, 26599, 350, 29941, 29896, 29896, 29899, 29906, 29906, 29896, 613, 376, 26022, 644, 22339, 21463, 4820, 365, 2190, 1115, 376, 29896, 3354, 29932, 19602, 376, 26022, 644, 22339, 21463, 4820, 399, 2190, 1115, 376, 29896, 3354, 29932, 19602, 376, 26022, 644, 22339, 14519, 12087, 29921, 1115, 376, 29896, 613, 376, 30032, 16279, 4570, 1488, 676, 1115, 376, 29896, 29947, 15800, 613, 376, 30012, 29982, 2223, 676, 4570, 1488, 676, 1115, 376, 29906, 15800, 613, 376, 30074, 29917, 24436, 4570, 1488, 676, 1115, 376, 29896, 29947, 15800, 613, 376, 30041, 29917, 29964, 14519, 12087, 933, 1115, 376, 29942, 821, 30002, 23620, 613, 376, 30013, 10285, 8776, 12854, 29871, 29941, 29954, 29914, 29946, 29954, 2569, 1216, 1155, 1115, 376, 840, 613, 376, 30014, 9530, 29964, 13009, 676, 5274, 1115, 376, 576, 29960, 5460, 29936, 14519, 12087, 477, 29871, 29945, 757, 29936, 20468, 29899, 15266, 676, 29908, 500, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 1, 1788, 13, 30054, 29972, 29942, 753, 717, 1097, 7678, 3378, 3327, 18077, 494, 490, 9116, 1155, 730, 4390, 1866, 614, 7832, 1587, 606, 614, 29969, 5206, 6052, 23092, 24871, 665, 8487, 27268, 29964, 6188, 1502, 29889, 939, 1685, 7616, 2771, 1521, 956, 1097, 7678, 3378, 3327, 18077, 494, 606, 11244, 6253, 12395, 29889, 2, 1, 1404, 13, 30038, 29969, 5206, 6195, 29901, 6083, 29960, 5460, 379, 3357, 26599, 350, 29941, 29896, 29896, 29899, 29906, 29906, 29896, 531, 1447, 9518, 25163, 2082, 21944, 30002, 8150, 14519, 12087, 2082, 718, 1325, 29959, 10766, 676, 13, 30038, 7832, 1755, 29901, 29871, 2014, 4222, 24178, 7372, 2370, 16042, 11744, 29901, 14570, 29899, 18800, 29899, 576, 29960, 5460, 718, 1447, 9518, 25163, 3162, 14519, 12087, 477, 29871, 29945, 757, 718, 20468, 29899, 15266, 676, 29889, 6083, 29960, 5460, 26780, 19945, 29932, 3807, 21656, 717, 317, 7833, 29899, 15266, 3327, 606, 20302, 1604, 4449, 2100, 1499, 1413, 25827, 24050, 29899, 1268, 29957, 8776, 12854, 1447, 29871, 29941, 29900, 836, 4449, 29964, 7735, 29957, 2430, 863, 21237, 2513, 3288, 2835, 13145, 570, 29889, 13, 13, 30082, 702, 29932, 16042, 11744, 14367, 1084, 4820, 16970, 1909, 2237, 8751, 3807, 2771, 26352, 490, 25827, 24050, 490, 11425, 27596, 29988, 29892, 11270, 1685, 20684, 25251, 18493, 29957, 2082, 25827, 24050, 29889, 6083, 29960, 5460, 26400, 13204, 1257, 6457, 1500, 5413, 531, 18493, 29957, 4470, 25827, 821, 4816, 29892, 4553, 606, 531, 2569, 2384, 693, 4470, 25827, 821, 4816, 13068, 20468, 29899, 15266, 1500, 29889, 20782, 9253, 21487, 570, 20356, 1500, 4393, 570, 29892, 1694, 12068, 18493, 29957, 2082, 25827, 24050, 19568, 1257, 531, 2942, 1782, 26344, 7082, 490, 516, 1502, 1685, 20684, 25251, 29889, 13, 13, 30027, 29904, 29960, 5460, 26400, 13204, 1257, 1586, 811, 29871, 29941, 29954, 29892, 29871, 29946, 29954, 29892, 365, 4330, 29871, 29946, 28868, 588, 5887, 606, 26780, 29871, 29906, 21463, 676, 29901, 365, 2190, 606, 399, 2190, 29889, 16456, 6517, 26400, 13204, 1257, 16223, 29959, 1909, 1668, 6713, 606, 14887, 2584, 2542, 24712, 693, 29889, 26595, 5508, 1561, 29960, 6713, 29901, 662, 16279, 29871, 29896, 29947, 15800, 29892, 14141, 24436, 29871, 29896, 29941, 15800, 29892, 15804, 676, 29871, 29906, 15800, 29889, 13, 13, 30012, 16042, 13009, 730, 531, 1561, 29960, 730, 4416, 28866, 29932, 21944, 30002, 23620, 863, 1325, 2510, 3162, 14519, 12087, 477, 531, 3212, 30055, 29919, 17795, 317, 1529, 606, 662, 644, 2082, 1787, 3759, 1225, 29871, 29945, 17194, 2899, 29889, 7473, 29959, 29899, 15266, 676, 2102, 1802, 3843, 1225, 4364, 490, 12493, 16635, 29892, 606, 1710, 641, 30011, 19568, 1257, 733, 6495, 29977, 11325, 665, 4724, 4791, 16458, 9011, 10744, 753, 29906, 490, 531, 1257, 13053, 29871, 29941, 29954, 29914, 29871, 29946, 29954, 29889, 7127, 1802, 3843, 1225, 4364, 25827, 24050, 29899, 6711, 2885, 29951, 29871, 29906, 29900, 29900, 1618, 30031, 10642, 614, 2194, 9578, 2332, 733, 18924, 576, 1415, 606, 665, 3212, 840, 9797, 29892, 18068, 5719, 29932, 1538, 1787, 1282, 1257, 29889, 1291, 1782, 29921, 3648, 4484, 8433, 676, 21500, 29871, 29929, 29900, 29900, 5368, 29975, 6188, 490, 29706, 29996, 29889, 13, 13, 30013, 1719, 29957, 17442, 8751, 23842, 4647, 1630, 3693, 15757, 956, 1413, 11650, 3029, 665, 7391, 2937, 12394, 1779, 29988, 29889, 20782, 20302, 23124, 1413, 1685, 24356, 1447, 8847, 29988, 6457, 1499, 29988, 662, 8150, 29889, 12485, 3693, 15757, 18479, 11650, 2430, 26634, 733, 9518, 507, 1413, 4724, 4453, 29935, 606, 7140, 1413, 18636, 9718, 4199, 25827, 821, 4816, 29889, 6049, 4453, 29935, 665, 10766, 730, 21500, 29871, 29900, 5368, 29975, 6188, 29889, 13, 13, 20093, 2711, 1844, 29977, 1229, 29892, 1538, 20345, 29978, 730, 11366, 12735, 30005, 614, 14236, 1382, 1345, 5392, 2237, 22951, 29889, 2081, 570, 9077, 22568, 29932, 733, 11414, 29975, 1263, 4199, 20658, 2510, 3029, 665, 21237, 717, 29892, 606, 26907, 23618, 29959, 1345, 4184, 14752, 19943, 8935, 29959, 13068, 3212, 1216, 29944, 376, 27129, 5945, 5551, 29908, 7082, 11425, 1268, 29957, 6620, 29998, 1382, 29889, 2, 1, 9225, 13, 29912, 376, 30017, 29904, 13538, 1115, 376, 29950, 3357, 26599, 350, 29941, 29896, 29896, 29899, 29906, 29906, 29896, 613, 376, 26022, 644, 22339, 21463, 4820, 365, 2190, 1115, 376, 29896, 3354, 29932, 19602, 376, 26022, 644, 22339, 21463, 4820, 399, 2190, 1115, 376, 29896, 3354, 29932, 19602, 376, 26022, 644, 22339, 14519, 12087, 29921, 1115, 376, 29896, 613, 376, 30032, 16279, 4570, 1488, 676, 1115, 376, 29896, 29947, 15800, 613, 376, 30012, 29982, 2223, 676, 4570, 1488, 676, 1115, 376, 29906, 15800, 613, 376, 30074, 29917, 24436, 4570, 1488, 676, 1115, 376, 29896, 29947, 15800, 613, 376, 30041, 29917, 29964, 14519, 12087, 933, 1115, 376, 29942, 821, 30002, 23620, 613, 376, 30013, 10285, 8776, 12854, 29871, 29941, 29954, 29914, 29946, 29954, 2569, 1216, 1155, 1115, 376, 840, 613, 376, 30014, 9530, 29964, 13009, 676, 5274, 1115, 376, 576, 29960, 5460, 29936, 14519, 12087, 477, 29871, 29945, 757, 29936, 20468, 29899, 15266, 676, 29908, 500, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(generate_and_tokenize_prompt(data[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aacff490-5e62-46f9-97e6-4118e122366a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:21.860135Z",
     "iopub.status.busy": "2024-05-21T22:12:21.859207Z",
     "iopub.status.idle": "2024-05-21T22:12:24.801325Z",
     "shell.execute_reply": "2024-05-21T22:12:24.800435Z",
     "shell.execute_reply.started": "2024-05-21T22:12:21.860114Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 404.57 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 411.55 examples/s]\n",
      "Map: 100%|██████████| 1045/1045 [00:02<00:00, 432.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = (\n",
    "    data[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "\n",
    "val_data = (\n",
    "    data[\"validation\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "\n",
    "test_data = (\n",
    "    data[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0463292-ce4a-4c5e-ae70-411074fb8a08",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26d38a2f-754d-4956-9b59-86c52e259801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:24.804886Z",
     "iopub.status.busy": "2024-05-21T22:12:24.804533Z",
     "iopub.status.idle": "2024-05-21T22:12:24.813708Z",
     "shell.execute_reply": "2024-05-21T22:12:24.813134Z",
     "shell.execute_reply.started": "2024-05-21T22:12:24.804865Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55f62da4-0698-4618-910d-74c4fd69a278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:24.814797Z",
     "iopub.status.busy": "2024-05-21T22:12:24.814328Z",
     "iopub.status.idle": "2024-05-21T22:12:24.843052Z",
     "shell.execute_reply": "2024-05-21T22:12:24.842145Z",
     "shell.execute_reply.started": "2024-05-21T22:12:24.814770Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "MICRO_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 200\n",
    "OUTPUT_DIR = \"/home/jupyter/datasphere/project/tmp/model\"\n",
    "\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "#             warmup_steps=200,\n",
    "            max_steps=TRAIN_STEPS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            logging_strategy = \"steps\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=10,\n",
    "            save_steps=10,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            save_total_limit=20,\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=None,\n",
    "            overwrite_output_dir=True, # Overwrite the content of the output dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "318d2bfb-7d97-420a-9848-7f813f03b6f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:12:24.845087Z",
     "iopub.status.busy": "2024-05-21T22:12:24.844234Z",
     "iopub.status.idle": "2024-05-21T22:12:24.854931Z",
     "shell.execute_reply": "2024-05-21T22:12:24.854262Z",
     "shell.execute_reply.started": "2024-05-21T22:12:24.845063Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0306cb8e-b014-4447-9be7-df58f33edd26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:18:43.092457Z",
     "iopub.status.busy": "2024-05-21T22:18:43.092019Z",
     "iopub.status.idle": "2024-05-21T22:18:43.127500Z",
     "shell.execute_reply": "2024-05-21T22:18:43.126806Z",
     "shell.execute_reply.started": "2024-05-21T22:18:43.092429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_file = '/home/jupyter/datasphere/project/tmp/checkpoint-100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5cd9cf47-5545-4f1c-aae5-32dad86f2dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:21:34.838025Z",
     "iopub.status.busy": "2024-05-21T22:21:34.837588Z",
     "iopub.status.idle": "2024-05-21T22:21:35.615849Z",
     "shell.execute_reply": "2024-05-21T22:21:35.614750Z",
     "shell.execute_reply.started": "2024-05-21T22:21:34.838002Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "/home/jupyter/datasphere/project/tmp is not a folder containing a `.index.json` file or a pytorch_model.bin or a model.safetensors file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12118/2207876843.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/jupyter/datasphere/project/tmp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = load_checkpoint_and_dispatch(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mload_checkpoint_and_dispatch\u001b[0;34m(model, checkpoint, device_map, max_memory, no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict, skip_keys, preload_module_classes, force_hooks, strict)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moffload_state_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0moffload_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m     load_checkpoint_in_model(\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mload_checkpoint_in_model\u001b[0;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb, strict)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mpotential_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".index.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotential_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1638\u001b[0m                     \u001b[0;34mf\"{checkpoint} is not a folder containing a `.index.json` file or a {WEIGHTS_NAME} or a {SAFE_WEIGHTS_NAME} file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: /home/jupyter/datasphere/project/tmp is not a folder containing a `.index.json` file or a pytorch_model.bin or a model.safetensors file"
     ]
    }
   ],
   "source": [
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "checkpoint_file = '/home/jupyter/datasphere/project/tmp'\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model, checkpoint=checkpoint_file, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f60eb45d-53ce-4472-b289-1276fdd1c7ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:17:13.116399Z",
     "iopub.status.busy": "2024-05-21T22:17:13.115994Z",
     "iopub.status.idle": "2024-05-21T22:17:13.140100Z",
     "shell.execute_reply": "2024-05-21T22:17:13.139140Z",
     "shell.execute_reply.started": "2024-05-21T22:17:13.116377Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Извлеки атрибуты товара в формате json из описания и оглавления продукта на маркетплейсе. В ответ выведи атрибуты товара и их значения.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]['system']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c29a284e-0328-41e1-a5e4-e1456a5bef86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:20:44.109597Z",
     "iopub.status.busy": "2024-05-21T22:20:44.108397Z",
     "iopub.status.idle": "2024-05-21T22:20:53.957374Z",
     "shell.execute_reply": "2024-05-21T22:20:53.956573Z",
     "shell.execute_reply.started": "2024-05-21T22:20:44.109571Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оглавление: Смартфон OPPO F5 CPH1723 6/128 GB (золотистый)\n",
      "Описание: Смартфон OPPO F5 CPH1723 получил 6-дюймовый дисплей, который позволяет просматривать фильмы, читать книги и находить нужную информацию в интернете, не напрягая глаз. Несмотря на это, он остался легким, компактным и очень удобным в использовании. Причиной тому стали ультратонкие рамки, которые остаются практически незаметными. К числу прогрессивных решений стоит отнести и соотношение сторон 2:1. Оно позволяет запускать два приложения одновременно, не отрываясь от просмотра фильма при отправке сообщения или загрузке файлов.2.5D стекло добавляет дополнительную эстетику и ощущение удобства при использовании Производительность OPPO F5 CPH1723 обеспечивается восьмиядерным процессором MediaTek MT6763T с тактовой частотой 2.5 ГГц. С 6 ГБ оперативной памяти и встроенной памятью объемом 128 ГБ, у вас есть достаточно ресурсов для плавной работы приложений и хранения данных.Основная камера с разрешением 16 Мпикс и встроенной вспышкой позволяет создавать качественные фотографии и записывать видео в разрешении FullHD. Фронтальная камера на 20 Мпикс сделана для любителей селфи и видеозвонков. OPPO F5 CPH1723 - это смартфон, который предлагает выдающиеся характеристики, сочетая в себе стильный дизайн и продвинутые технологии. Модель работает на операционной системе Android 7.1.Смартфон поддерживает использование двух nano-SIM-карт, а также расширение памяти с помощью карты microSD объемом до 256 ГБ. Встроенный сканер отпечатка пальца ускоряет разблокирование экрана и обеспечивает должный уровень безопасности при подтверждении электронных платежей. Устройство поддерживает и альтернативный способ авторизации – трехмерное сканирование лица с помощью фронтальной камеры. Фирменная оболочка ОС ColorOS 3.2 предлагает пользователю ряд интересных функций – в том числе ускоренную передачу файлов, специальный игровой режим с заблокированными уведомлениями, персонализацию интерфейса и уменьшение нагрузки на глаза при чтении в вечернее время.\n",
      "{ \"name\": \"OPPO F5 CPH1723\", \"description\": \"Смартфон OPPO F5 CPH1723 имеет 6 дюймовый экран, который позволит вам просматривать фильмы, читать книги и найти нужному в Интернете без напряжения глаз. Несмотря на это, он остается легким, компактным и очень удобен в использовании.\", \"og_title\": \"OPPO F5 CPH1723 6/12GB Gold\" }\n",
      "CPU times: user 9.83 s, sys: 31.7 ms, total: 9.87 s\n",
      "Wall time: 9.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inp = val_data[3]['user']\n",
    "conversation = Conversation(system_prompt=val_data[3]['system'])\n",
    "conversation.add_user_message(inp)\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "print(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eef1b6a4-d9d7-4a28-a18c-a696f01f5b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:20:00.376192Z",
     "iopub.status.busy": "2024-05-21T22:20:00.375082Z",
     "iopub.status.idle": "2024-05-21T22:20:00.455246Z",
     "shell.execute_reply": "2024-05-21T22:20:00.454121Z",
     "shell.execute_reply.started": "2024-05-21T22:20:00.376163Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12118/1748251891.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = transformers.Trainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_arguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# Quantized models doesn't support `.to` operation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_quantized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd42f8e8-f562-4ce6-9991-65aaf2bba3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:19:39.643256Z",
     "iopub.status.busy": "2024-05-21T22:19:39.642836Z",
     "iopub.status.idle": "2024-05-21T22:19:39.675237Z",
     "shell.execute_reply": "2024-05-21T22:19:39.674025Z",
     "shell.execute_reply.started": "2024-05-21T22:19:39.643232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find a checkpoint index (pytorch_model.bin.index.json or model.safetensors.index.json) in /home/jupyter/datasphere/project/tmp/checkpoint-170.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12118/2358372493.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jupyter/datasphere/project/tmp/checkpoint-170'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_deepspeed_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0;31m# If model was re-initialized, put it on the right device and update self.model_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(self, resume_from_checkpoint, model)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m             \u001b[0;31m# We load the sharded checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2086\u001b[0;31m             load_result = load_sharded_checkpoint(\n\u001b[0m\u001b[1;32m   2087\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_safetensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_sharded_checkpoint\u001b[0;34m(model, folder, strict, prefer_safe)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mWEIGHTS_INDEX_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAFE_WEIGHTS_INDEX_NAME\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_safetensors_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mWEIGHTS_INDEX_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         )\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find a checkpoint index ({' or '.join(filenames)}) in {folder}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0mload_safe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find a checkpoint index (pytorch_model.bin.index.json or model.safetensors.index.json) in /home/jupyter/datasphere/project/tmp/checkpoint-170."
     ]
    }
   ],
   "source": [
    "trainer.train('/home/jupyter/datasphere/project/tmp/checkpoint-170')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6466277-728b-47bb-803e-29a6bfbd7164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T22:19:47.465230Z",
     "iopub.status.busy": "2024-05-21T22:19:47.464387Z",
     "iopub.status.idle": "2024-05-21T22:19:47.492675Z",
     "shell.execute_reply": "2024-05-21T22:19:47.491620Z",
     "shell.execute_reply.started": "2024-05-21T22:19:47.465206Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12118/2245482801.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jupyter/datasphere/project/tmp/model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('/home/jupyter/datasphere/project/tmp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0580c6-6a53-488f-957a-713fc3993bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "#model = load_checkpoint_and_dispatch(\n",
    "#    model, checkpoint=checkpoint_file, device_map=\"auto\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de93ca-8f0f-4d12-a483-29b03892a5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model = torch.compile(model)\n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e26b2-1d0e-4ef5-8825-d6330f94567b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-21T12:32:29.972430Z",
     "iopub.status.idle": "2024-05-21T12:32:29.972694Z",
     "shell.execute_reply": "2024-05-21T12:32:29.972579Z",
     "shell.execute_reply.started": "2024-05-21T12:32:29.972568Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()\n",
    "model.push_to_hub(\"BaronFonMonc/Saiga_2_7b_fine_tune_wb\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee1b9e-3c88-43ee-90d0-8ce4fb4a18e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39a61853-3769-46fd-9e6a-0febb0b450fc",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1f138-249e-4071-bd00-2831b5acb25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_instruct_dir = '/home/jupyter/datasphere/project/tmp/rulm/self_instruct'\n",
    "\n",
    "checkpoint = \"/home/jupyter/datasphere/project/tmp/checkpoint-60\"\n",
    "\n",
    "merged_model_name = '/home/jupyter/datasphere/project/tmp/merged_test_model.pt'\n",
    "\n",
    "self_instruct_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5818fc0-c45a-42e5-83c8-c5cfea4ddab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {self_instruct_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf792e9c-c0cd-4543-8c80-ac88e0a0f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bf109-23a8-4139-9077-acdbbf5aecfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5755053-0a7f-4dd5-bfb7-3f6926a53425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python \n",
    "python -m src.tools.convert_to_native {checkpoint} {merged_model_name} --device=cuda --enable_offloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f7156-a09b-490a-b627-6396ada3f89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model_name.endswith('.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d283a82-55f3-4156-b06a-a489d56e9066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00) # Перезапустить ядро"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196a2cf-cd20-453d-8fa9-9234b919e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_instruct_dir = '/home/jupyter/datasphere/project/tmp/rulm/self_instruct'\n",
    "\n",
    "checkpoint = \"/home/jupyter/datasphere/project/tmp/checkpoint-60\"\n",
    "\n",
    "merged_model_name = '/home/jupyter/datasphere/project/tmp/merged_test_model.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b97f4-5c95-4cd0-abd5-6cd9665cc7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {self_instruct_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8123db0-10ce-4887-8159-8b6e2283355a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m src.tools.convert_to_native '/home/jupyter/datasphere/project/tmp/checkpoint-60' '/home/jupyter/datasphere/project/tmp/merged_test_model.pt' --device=cuda --enable_offloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80efe2-14f8-4739-bc17-76beee9a7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fbea3-f1d9-400e-94e9-e6a357afed9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m src.tools.convert_to_native {checkpoint} {merged_model_name} --device=cuda --enable_offloading\n",
    "# assert (output_dir / merged_model_name).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075b69a-d13d-42ac-83b1-7c128437c115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /home/jupyter/datasphere/project/tmp\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c6efa-e9a1-4986-a021-11ae332aac09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga2_7b_lora\", use_fast=False)\n",
    "tokenizer.save_pretrained('/home/jupyter/datasphere/project/tmp/checkpoint-60')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a9951-576c-471d-aae4-856484e9657b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /home/jupyter/datasphere/project/tmp/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828fdadf-11ff-48e3-b5eb-f9126d1b007b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfebae-db81-4ebb-b149-7b7b4d91b364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08aacd-d461-484f-aa3a-1a2eb8a2bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_instruct_dir = '/home/jupyter/datasphere/project/tmp/rulm/self_instruct'\n",
    "\n",
    "checkpoint = \"/home/jupyter/datasphere/project/tmp/checkpoint-60\"\n",
    "\n",
    "merged_model_name = '/home/jupyter/datasphere/project/tmp/merged_test_model.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecfa27d-e310-4adc-8701-15f221e7f581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = '/home/jupyter/datasphere/project/tmp/merged_test_model.pt'\n",
    "checkpoint = \"/home/jupyter/datasphere/project/tmp/checkpoint-60\"\n",
    "output_model = \"/home/jupyter/datasphere/project/tmp/model-f16.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293666a-3407-4475-9a2d-28fdf057fe60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 convert.py '/home/jupyter/datasphere/project/tmp/merged_test_model.pt' --vocab-dir  '/home/jupyter/datasphere/project/tmp/checkpoint-60' --outfile '/home/jupyter/datasphere/project/tmp/model-f16.gguf' --outtype f16 --ctx 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1d9f7-ffda-40a8-b7df-26132c890a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python convert.py {model_dir} --vocab-dir {checkpoint} --outfile {output_model} --outtype f16 --ctx 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a020a15-5c89-40e9-a79c-2959028eab0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a3fbf-6736-4de5-a6bf-70ee7e754afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga2_7b_lora\", use_fast=False)\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32579f83-fc28-4901-8931-77520c6fd02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/tmp/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e28c6-8ba5-43fb-84f2-8e2d67f74c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_instruct_dir = '/home/jupyter/datasphere/project/tmp/rulm/self_instruct'\n",
    "\n",
    "checkpoint = \"/home/jupyter/datasphere/project/tmp/checkpoint-60\"\n",
    "\n",
    "merged_model_name = '/home/jupyter/datasphere/project/tmp/merged_test_model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388dbc0-a909-4fac-b259-79a17fa2384f",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6420f0-1731-4b2b-a9d4-1575c4c1819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter/datasphere/project/tmp/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e67ff-3e5f-4524-b379-247098eed22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f474555-967f-432d-88cf-3dddb9ba52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gguf = \"/home/jupyter/datasphere/project/tmp/model-f16.gguf\"\n",
    "quant_model = \"/home/jupyter/datasphere/project/tmp/model-q4_0.gguf\"\n",
    "quantization_type = \"q4_0\" #@param [\"q4_0\", \"q4_1\"] {allow-input: true}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b9561-6148-4473-ab81-de4bdb75cffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./quantize '/home/jupyter/datasphere/project/tmp/model-f16.gguf' '/home/jupyter/datasphere/project/tmp/model-q4_0.gguf' 'q4_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87916946-0a91-407c-87cc-36167b519982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd ~\n",
    "%cd /home/jupyter/datasphere/project/tmp/llama.cpp\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0069d-eb62-4174-9222-bf1e932eeaa7",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ee575-6d41-4f5a-b068-21d42e96377c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /home/jupyter/datasphere/project/tmp/\n",
    "!git clone --recursive https://github.com/ggerganov/llama.cpp.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c937477-3991-488c-b4e2-fa75900bbdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /home/jupyter/datasphere/project/tmp/llama.cpp\n",
    "!make LLAMA_CUBLAS=1 -j libllama.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf67b19-0ba2-41e2-8d58-e4034705d50b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HACK: Use custom compiled libllama.so\n",
    "%cp ~/llama.cpp/libllama.so /home/jupyter/datasphere/project/tmp/libllama.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006a602-a3b6-4cdf-b50f-ead0c01c4cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import fire\n",
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66a0b5-5a9f-42db-b255-20592308d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"Ты извлекаешь термины и определения из текста\"\n",
    "SYSTEM_TOKEN = 1788\n",
    "USER_TOKEN = 1404\n",
    "BOT_TOKEN = 9225\n",
    "LINEBREAK_TOKEN = 13\n",
    "\n",
    "top_k=40\n",
    "top_p=0.5\n",
    "temperature=0.01\n",
    "repeat_penalty=1.1\n",
    "\n",
    "\n",
    "ROLE_TOKENS = {\n",
    "    \"user\": USER_TOKEN,\n",
    "    \"bot\": BOT_TOKEN,\n",
    "    \"system\": SYSTEM_TOKEN\n",
    "}\n",
    "\n",
    "\n",
    "def get_message_tokens(model, role, content):\n",
    "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
    "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
    "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
    "    message_tokens.append(model.token_eos())\n",
    "    return message_tokens\n",
    "\n",
    "\n",
    "def get_system_tokens(model):\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT\n",
    "    }\n",
    "    return get_message_tokens(model, **system_message)\n",
    "\n",
    "def chat_saiga(message, model):\n",
    "    system_tokens = get_system_tokens(model)\n",
    "    tokens = system_tokens\n",
    "    # model.eval(tokens)\n",
    "    \n",
    "    message_tokens = get_message_tokens(model=model, role=\"user\", content=message)\n",
    "    role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
    "    tokens += message_tokens + role_tokens\n",
    "    # print(tokens)\n",
    "    # detokenize = model.detokenize(tokens)\n",
    "    # print(model.tokenize(full_prompt))\n",
    "    generator = model.generate(\n",
    "        tokens,\n",
    "        top_k = top_k,\n",
    "        top_p = top_p,\n",
    "        temp = temperature,\n",
    "        repeat_penalty = repeat_penalty,\n",
    "        reset = True\n",
    "    )\n",
    "    # print(len([token for token in generator]))\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for token in generator:\n",
    "        token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n",
    "        tokens.append(token)\n",
    "        if token == model.token_eos():\n",
    "            break\n",
    "        print(token_str, end=\"\", flush=True)\n",
    "        result_list.append(token_str)\n",
    "    return ''.join(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d08a3-55cf-4c0a-be08-24b034109381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%ls /home/jupyter/datasphere/project/tmp/llama.cpp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc4cd6-2c9f-49a8-bf70-ee45db85cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter/datasphere/project/tmp\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcd805-abee-482c-a68c-0fa3c82a9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jupyter/datasphere/project/tmp\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0304694-a994-4671-b812-4b89a5e94f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp ~/llama.cpp/libllama.so /home/jupyter/datasphere/project/tmp/libllama.so123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a24a8-d266-4a35-87bb-f1208bcdae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# model_path = '/kaggle/working/model-q4_0.gguf'\n",
    "model_path = '/kaggle/working/model-q4_0.gguf'\n",
    "n_ctx = 3096 #\n",
    "\n",
    "model = Llama(\n",
    "        model_path = model_path,\n",
    "        n_ctx = n_ctx,\n",
    "        n_gpu_layers=-1\n",
    ") # n_gpu_layers=-1  параметр для переноса модели на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbb01b-cfc0-43fe-9925-614082ec495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "promt = \"\"\"\n",
    "Извлеки из данного текста все определения и термины.\n",
    "Вывод должен быть в виде списка словарей с ключами term и definition\n",
    "\n",
    "Машинное обучение (ML) — это использование математических моделей данных, которые помогают компьютеру обучаться без непосредственных инструкций.\n",
    "Оно считается одной из форм искусственного интеллекта (ИИ).\n",
    "\"\"\"\n",
    "chat_saiga(promt, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90972984-18c1-40e3-918d-c4c6a5f89012",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c281d2e-b28b-465b-b972-a264aa72004b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты извлекаешь термины и определения из текста\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template = DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt = DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template = DEFAULT_RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt,\n",
    "                     return_tensors=\"pt\",\n",
    "                     add_special_tokens=False,\n",
    "#                      padding=True,\n",
    "#                     truncation=True\n",
    "                    )\n",
    "    data = {k: v.to(device) for k, v in data.items()}\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config = generation_config,\n",
    "#         remove_invalid_values = True\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f458b-6ffc-4b71-8eac-650c97fae8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_message(inputs):\n",
    "    result = []\n",
    "    for inp in inputs:\n",
    "        conversation = Conversation()\n",
    "        conversation.add_user_message(inp)\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        print(prompt)\n",
    "        output = generate(model, tokenizer, prompt, generation_config)\n",
    "        result.append(output)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8ba10-4116-49d2-9a73-9f5e8b0a0972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = \"\"\"\n",
    "Извлеки из данного текста все определения и термины.\n",
    "Вывод должен быть в виде списка словарей с ключами term и definition\n",
    "\n",
    "Линейная модель — модель, отображающая состояние или функционирование системы таким образом, что все взаимозависимости в ней принимаются линейными (см. Линейная зависимость, Линейность в экономике). Соответственно, она может формулироваться в виде одного линейного уравнения или системы линейных уравнений.\n",
    "\"\"\"\n",
    "get_message([p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffa51d-7a6e-4f78-b76a-09ba7c39f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0af5b-3343-4288-944a-34001a6ed2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()\n",
    "model.push_to_hub(\"BaronFonMonc/Saiga_2_7b_fine_tune_wb\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ce956-1d6e-4831-b820-7e66e809eba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
